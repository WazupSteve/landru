{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "editable": true,
        "id": "do-xZ-8B7nVH",
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import tianshou as ts\n",
        "\n",
        "# env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uxsim import *\n",
        "import itertools\n",
        "\n",
        "class TrafficSim(gym.Env):\n",
        "    def __init__(self, max_steps=3600, operation_timestep_width=10):\n",
        "        \"\"\"\n",
        "        traffic scenario: 4 signalized intersections as shown below:\n",
        "                N1  N2\n",
        "                |   |\n",
        "            W1--I1--I2--E1\n",
        "                |   |\n",
        "            W2--I3--I4--E2\n",
        "                |   |\n",
        "                S1  S2\n",
        "        Traffic demand is generated from each boundary node to all other boundary nodes.\n",
        "        action: to determine which direction should have greenlight for every 10 seconds for each intersection. 16 actions.\n",
        "            action 1: greenlight for I1: direction 0, I2: 0, I3: 0, I4: 0, where direction 0 is E<->W, 1 is N<->S.\n",
        "            action 2: greenlight for I1: 1, I2: 0, I3: 0, I4: 0\n",
        "            action 3: greenlight for I1: 0, I2: 1, I3: 0, I4: 0\n",
        "            action 4: greenlight for I1: 1, I2: 1, I3: 0, I4: 0\n",
        "            action 5: ...\n",
        "        state: number of waiting vehicles at each incoming link. 16 dimension.\n",
        "        reward: negative of difference of total waiting vehicles\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.spec = gym.envs.registration.EnvSpec('TrafficSim-4X')\n",
        "\n",
        "        self.spec.reward_threshold = float('inf')\n",
        "        self.spec.max_episode_steps = max_steps\n",
        "\n",
        "        #action\n",
        "        self.n_action = 2**4\n",
        "        self.action_space = gym.spaces.Discrete(self.n_action) \n",
        "        \n",
        "        #state\n",
        "        self.n_state = 4*4\n",
        "        low = np.array([0 for i in range(self.n_state)])\n",
        "        high = np.array([100 for i in range(self.n_state)])\n",
        "        self.observation_space = gym.spaces.Box(low=low, high=high)\n",
        "        \n",
        "        self.operation_timestep_width = operation_timestep_width\n",
        "\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        reset the env\n",
        "        \"\"\"\n",
        "        seed = None #whether demand is always random or not\n",
        "        W = World(\n",
        "            name=\"\",\n",
        "            deltan=5,\n",
        "            tmax=self.spec.max_episode_steps,\n",
        "            print_mode=0, save_mode=1, show_mode=1,\n",
        "            random_seed=seed,\n",
        "            duo_update_time=600\n",
        "        )\n",
        "        random.seed(seed)\n",
        "\n",
        "        #network definition\n",
        "        I1 = W.addNode(\"I1\", 0, 0, signal=[60,60])\n",
        "        I2 = W.addNode(\"I2\", 1, 0, signal=[60,60])\n",
        "        I3 = W.addNode(\"I3\", 0, -1, signal=[60,60])\n",
        "        I4 = W.addNode(\"I4\", 1, -1, signal=[60,60])\n",
        "        W1 = W.addNode(\"W1\", -1, 0)\n",
        "        W2 = W.addNode(\"W2\", -1, -1)\n",
        "        E1 = W.addNode(\"E1\", 2, 0)\n",
        "        E2 = W.addNode(\"E2\", 2, -1)\n",
        "        N1 = W.addNode(\"N1\", 0, 1)\n",
        "        N2 = W.addNode(\"N2\", 1, 1)\n",
        "        S1 = W.addNode(\"S1\", 0, -2)\n",
        "        S2 = W.addNode(\"S2\", 1, -2)\n",
        "        #E <-> W direction: signal group 0\n",
        "        for n1,n2 in [[W1, I1], [I1, I2], [I2, E1], [W2, I3], [I3, I4], [I4, E2]]:\n",
        "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
        "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
        "        #N <-> S direction: signal group 1\n",
        "        for n1,n2 in [[N1, I1], [I1, I3], [I3, S1], [N2, I2], [I2, I4], [I4, S2]]:\n",
        "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
        "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
        "\n",
        "        # random demand definition\n",
        "        dt = 30\n",
        "        demand = 0.22\n",
        "        for n1, n2 in itertools.permutations([W1, W2, E1, E2, N1, N2, S1, S2], 2):\n",
        "            for t in range(0, 3600, dt):\n",
        "                W.adddemand(n1, n2, t, t+dt, random.uniform(0, demand))\n",
        "        \n",
        "        # store UXsim object for later re-use\n",
        "        self.W = W\n",
        "        self.I1 = I1\n",
        "        self.I2 = I2\n",
        "        self.I3 = I3\n",
        "        self.I4 = I4\n",
        "        self.INLINKS = list(self.I1.inlinks.values()) + list(self.I2.inlinks.values()) + list(self.I3.inlinks.values()) + list(self.I4.inlinks.values())\n",
        "        \n",
        "        #initial observation\n",
        "        observation = np.array([0 for i in range(self.n_state)])\n",
        "        \n",
        "        #log\n",
        "        self.log_state = []\n",
        "        self.log_reward = []\n",
        "        \n",
        "        return observation, {}\n",
        "    \n",
        "    def comp_state(self):\n",
        "        \"\"\"\n",
        "        compute the current state\n",
        "        \"\"\"\n",
        "        vehicles_per_links = {}\n",
        "        for l in self.INLINKS:\n",
        "            vehicles_per_links[l] = l.num_vehicles_queue #l.num_vehicles_queue: the number of vehicles in queue in link l\n",
        "        return list(vehicles_per_links.values())\n",
        "    \n",
        "    def comp_n_veh_queue(self):\n",
        "        return sum(self.comp_state())\n",
        "    \n",
        "    def step(self, action_index):\n",
        "        \"\"\"\n",
        "        proceed env by 1 step = `operation_timestep_width` seconds\n",
        "        \"\"\"\n",
        "        \n",
        "        n_queue_veh_old = self.comp_n_veh_queue()\n",
        "        \n",
        "        #change signal by action\n",
        "        #decode action\n",
        "        binstr = f\"{action_index:04b}\"\n",
        "        i1, i2, i3, i4 = int(binstr[3]), int(binstr[2]), int(binstr[1]), int(binstr[0])\n",
        "        self.I1.signal_phase = i1\n",
        "        self.I1.signal_t = 0\n",
        "        self.I2.signal_phase = i2\n",
        "        self.I2.signal_t = 0\n",
        "        self.I3.signal_phase = i3\n",
        "        self.I3.signal_t = 0\n",
        "        self.I4.signal_phase = i4\n",
        "        self.I4.signal_t = 0\n",
        "        \n",
        "        #traffic dynamics. execute simulation for `operation_timestep_width` seconds\n",
        "        if self.W.check_simulation_ongoing():\n",
        "            self.W.exec_simulation(duration_t=self.operation_timestep_width)\n",
        "        \n",
        "        #observe state\n",
        "        observation = np.array(self.comp_state())\n",
        "        \n",
        "        #compute reward\n",
        "        n_queue_veh = self.comp_n_veh_queue()\n",
        "        reward = -(n_queue_veh-n_queue_veh_old)\n",
        "        \n",
        "        #check termination\n",
        "        done = False\n",
        "        if self.W.check_simulation_ongoing() == False:\n",
        "            done = True\n",
        "        \n",
        "        #log\n",
        "        self.log_state.append(observation)\n",
        "        self.log_reward.append(reward)\n",
        "        \n",
        "        return observation, reward, done, False, {}\n",
        "    \n",
        "    def render(self) -> None:\n",
        "        self.W.analyzer.network(self.W.TIME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# train_envs = gym.make('CartPole-v1')\n",
        "# test_envs = gym.make('CartPole-v1')\n",
        "\n",
        "max_steps = 3600\n",
        "time_per_step = 10\n",
        "\n",
        "env = TrafficSim(max_steps, time_per_step)\n",
        "\n",
        "train_envs = TrafficSim(max_steps, time_per_step)\n",
        "test_envs  = TrafficSim(max_steps, time_per_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMUNPN5SI_kd",
        "outputId": "7d68323c-0322-4b82-dafb-7c7f63e7a26d"
      },
      "outputs": [],
      "source": [
        "# train_envs = ts.env.DummyVectorEnv([TrafficSim for _ in range(10)])\n",
        "# test_envs = ts.env.DummyVectorEnv([TrafficSim for _ in range(100)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch, numpy as np\n",
        "from torch import nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, np.prod(action_shape)),\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, state=None, info={}):\n",
        "        if not isinstance(obs, torch.Tensor):\n",
        "            obs = torch.tensor(obs, dtype=torch.float)\n",
        "        batch = obs.shape[0]\n",
        "        logits = self.model(obs.view(batch, -1))\n",
        "        return logits, state\n",
        "\n",
        "state_shape = env.observation_space.shape or env.observation_space.n\n",
        "action_shape = env.action_space.shape or env.action_space.n\n",
        "net = Net(state_shape, action_shape)\n",
        "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = ts.policy.DQNPolicy(\n",
        "    model=net,\n",
        "    optim=optim,\n",
        "    action_space=env.action_space,\n",
        "    discount_factor=0.9,\n",
        "    estimation_step=3,\n",
        "    target_update_freq=320\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: Single environment detected, wrap to DummyVectorEnv.\n"
          ]
        }
      ],
      "source": [
        "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, 1), exploration_noise=True)\n",
        "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CollectStats(n_collected_episodes=0, n_collected_steps=1, collect_time=0.006218433380126953, collect_speed=160.81220765278735, returns=array([], dtype=float64), returns_stat=None, lens=array([], dtype=int32), lens_stat=None)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_collector.collect(n_step=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_collector.env.get_env_attr('W')[0].TIME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #1:   1%|          | 2/360.0 [00:00<00:01, 249.97it/s]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "n_gradient_steps is 0, n_collected_steps=2, update_per_step=0.1",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[42], line 10\u001b[0m\n\u001b[0;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOffpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mtime_per_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_step\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_step\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmean_rewards\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_rewards\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_threshold\u001b[49m\n\u001b[1;32m---> 10\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training! Use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mtiming\u001b[38;5;241m.\u001b[39mtotal_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\trainer\\base.py:526\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m     \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[0;32m    527\u001b[0m     info \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[0;32m    528\u001b[0m         start_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time,\n\u001b[0;32m    529\u001b[0m         policy_update_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_update_time,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    534\u001b[0m         test_collector\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
            "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\trainer\\base.py:334\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m     train_stat \u001b[38;5;241m=\u001b[39m CollectStatsBase(\n\u001b[0;32m    330\u001b[0m         n_collected_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer),\n\u001b[0;32m    331\u001b[0m     )\n\u001b[0;32m    332\u001b[0m     t\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m--> 334\u001b[0m update_stat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_update_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_stat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m pbar_data_dict \u001b[38;5;241m=\u001b[39m set_numerical_fields_to_precision(pbar_data_dict)\n\u001b[0;32m    336\u001b[0m pbar_data_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgradient_step\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_step)\n",
            "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\trainer\\base.py:600\u001b[0m, in \u001b[0;36mOffpolicyTrainer.policy_update_fn\u001b[1;34m(self, collect_stats)\u001b[0m\n\u001b[0;32m    598\u001b[0m n_gradient_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_per_step \u001b[38;5;241m*\u001b[39m n_collected_steps)\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_gradient_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    601\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_gradient_steps is 0, n_collected_steps=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_collected_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    602\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_per_step=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_per_step\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    603\u001b[0m     )\n\u001b[0;32m    604\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_gradient_steps):\n\u001b[0;32m    605\u001b[0m     update_stat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample_and_update(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_collector\u001b[38;5;241m.\u001b[39mbuffer)\n",
            "\u001b[1;31mValueError\u001b[0m: n_gradient_steps is 0, n_collected_steps=2, update_per_step=0.1"
          ]
        }
      ],
      "source": [
        "result = ts.trainer.OffpolicyTrainer(\n",
        "    policy=policy,\n",
        "    train_collector=train_collector,\n",
        "    test_collector=test_collector,\n",
        "    max_epoch=10, step_per_epoch=max_steps/time_per_step, step_per_collect=2,\n",
        "    update_per_step=0.5, episode_per_test=10, batch_size=4,\n",
        "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
        "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
        "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold\n",
        ").run()\n",
        "print(f'Finished training! Use {result.timing.total_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: Single environment detected, wrap to DummyVectorEnv.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'W' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m policy\u001b[38;5;241m.\u001b[39mset_eps(\u001b[38;5;241m0.05\u001b[39m)\n\u001b[0;32m      3\u001b[0m collector \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mCollector(policy, env, exploration_noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m35\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\data\\collector.py:345\u001b[0m, in \u001b[0;36mCollector.collect\u001b[1;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_fn(\n\u001b[0;32m    334\u001b[0m             obs_next\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mobs_next,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    341\u001b[0m         ),\n\u001b[0;32m    342\u001b[0m     )\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m--> 345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(render, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    347\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(render)\n",
            "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\env\\venvs.py:345\u001b[0m, in \u001b[0;36mBaseVectorEnv.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_async \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_id) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are still stepping, cannot render them now.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    344\u001b[0m     )\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m]\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\env\\venvs.py:345\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_async \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_id) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    342\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    343\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are still stepping, cannot render them now.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    344\u001b[0m     )\n\u001b[1;32m--> 345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers]\n",
            "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\env\\worker\\dummy.py:52\u001b[0m, in \u001b[0;36mDummyEnvWorker.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[2], line 162\u001b[0m, in \u001b[0;36mTrafficSim.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39manalyzer\u001b[38;5;241m.\u001b[39mnetwork(\u001b[43mW\u001b[49m\u001b[38;5;241m.\u001b[39mTIME)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'W' is not defined"
          ]
        }
      ],
      "source": [
        "policy.eval()\n",
        "policy.set_eps(0.05)\n",
        "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
        "collector.collect(n_episode=1, render=1/35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "S3-tJZy35Ck_",
        "XfsuU2AAE52C",
        "p-7U_cwgF5Ej",
        "_j3aUJZQ7nml"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
