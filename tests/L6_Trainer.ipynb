{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "editable": true,
        "id": "do-xZ-8B7nVH",
        "slideshow": {
          "slide_type": ""
        }
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import tianshou as ts\n",
        "\n",
        "# env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from uxsim import *\n",
        "import itertools\n",
        "\n",
        "class TrafficSim(gym.Env):\n",
        "    def __init__(self, max_steps=3600, operation_timestep_width=10):\n",
        "        \"\"\"\n",
        "        traffic scenario: 4 signalized intersections as shown below:\n",
        "                N1  N2\n",
        "                |   |\n",
        "            W1--I1--I2--E1\n",
        "                |   |\n",
        "            W2--I3--I4--E2\n",
        "                |   |\n",
        "                S1  S2\n",
        "        Traffic demand is generated from each boundary node to all other boundary nodes.\n",
        "        action: to determine which direction should have greenlight for every 10 seconds for each intersection. 16 actions.\n",
        "            action 1: greenlight for I1: direction 0, I2: 0, I3: 0, I4: 0, where direction 0 is E<->W, 1 is N<->S.\n",
        "            action 2: greenlight for I1: 1, I2: 0, I3: 0, I4: 0\n",
        "            action 3: greenlight for I1: 0, I2: 1, I3: 0, I4: 0\n",
        "            action 4: greenlight for I1: 1, I2: 1, I3: 0, I4: 0\n",
        "            action 5: ...\n",
        "        state: number of waiting vehicles at each incoming link. 16 dimension.\n",
        "        reward: negative of difference of total waiting vehicles\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.spec = gym.envs.registration.EnvSpec('TrafficSim-4X')\n",
        "\n",
        "        self.spec.reward_threshold = float('inf')\n",
        "        self.spec.max_episode_steps = max_steps\n",
        "\n",
        "        #action\n",
        "        self.n_action = 2**4\n",
        "        self.action_space = gym.spaces.Discrete(self.n_action) \n",
        "        \n",
        "        #state\n",
        "        self.n_state = 4*4\n",
        "        low = np.array([0 for i in range(self.n_state)])\n",
        "        high = np.array([100 for i in range(self.n_state)])\n",
        "        self.observation_space = gym.spaces.Box(low=low, high=high)\n",
        "        \n",
        "        self.operation_timestep_width = operation_timestep_width\n",
        "\n",
        "        self.reset()\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        reset the env\n",
        "        \"\"\"\n",
        "        seed = None #whether demand is always random or not\n",
        "        W = World(\n",
        "            name=\"\",\n",
        "            deltan=5,\n",
        "            tmax=self.spec.max_episode_steps,\n",
        "            print_mode=0, save_mode=1, show_mode=0,\n",
        "            random_seed=seed,\n",
        "            duo_update_time=600\n",
        "        )\n",
        "        random.seed(seed)\n",
        "\n",
        "        #network definition\n",
        "        I1 = W.addNode(\"I1\", 0, 0, signal=[60,60])\n",
        "        I2 = W.addNode(\"I2\", 1, 0, signal=[60,60])\n",
        "        I3 = W.addNode(\"I3\", 0, -1, signal=[60,60])\n",
        "        I4 = W.addNode(\"I4\", 1, -1, signal=[60,60])\n",
        "        W1 = W.addNode(\"W1\", -1, 0)\n",
        "        W2 = W.addNode(\"W2\", -1, -1)\n",
        "        E1 = W.addNode(\"E1\", 2, 0)\n",
        "        E2 = W.addNode(\"E2\", 2, -1)\n",
        "        N1 = W.addNode(\"N1\", 0, 1)\n",
        "        N2 = W.addNode(\"N2\", 1, 1)\n",
        "        S1 = W.addNode(\"S1\", 0, -2)\n",
        "        S2 = W.addNode(\"S2\", 1, -2)\n",
        "        #E <-> W direction: signal group 0\n",
        "        for n1,n2 in [[W1, I1], [I1, I2], [I2, E1], [W2, I3], [I3, I4], [I4, E2]]:\n",
        "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
        "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
        "        #N <-> S direction: signal group 1\n",
        "        for n1,n2 in [[N1, I1], [I1, I3], [I3, S1], [N2, I2], [I2, I4], [I4, S2]]:\n",
        "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
        "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
        "\n",
        "        # random demand definition\n",
        "        dt = 30\n",
        "        demand = 0.22\n",
        "        for n1, n2 in itertools.permutations([W1, W2, E1, E2, N1, N2, S1, S2], 2):\n",
        "            for t in range(0, 3600, dt):\n",
        "                W.adddemand(n1, n2, t, t+dt, random.uniform(0, demand))\n",
        "        \n",
        "        # store UXsim object for later re-use\n",
        "        self.W = W\n",
        "        self.I1 = I1\n",
        "        self.I2 = I2\n",
        "        self.I3 = I3\n",
        "        self.I4 = I4\n",
        "        self.INLINKS = list(self.I1.inlinks.values()) + list(self.I2.inlinks.values()) + list(self.I3.inlinks.values()) + list(self.I4.inlinks.values())\n",
        "        \n",
        "        #initial observation\n",
        "        observation = np.array([0 for i in range(self.n_state)])\n",
        "        \n",
        "        #log\n",
        "        self.log_state = []\n",
        "        self.log_reward = []\n",
        "        \n",
        "        return observation, {}\n",
        "    \n",
        "    def comp_state(self):\n",
        "        \"\"\"\n",
        "        compute the current state\n",
        "        \"\"\"\n",
        "        vehicles_per_links = {}\n",
        "        for l in self.INLINKS:\n",
        "            vehicles_per_links[l] = l.num_vehicles_queue #l.num_vehicles_queue: the number of vehicles in queue in link l\n",
        "        return list(vehicles_per_links.values())\n",
        "    \n",
        "    def comp_n_veh_queue(self):\n",
        "        return sum(self.comp_state())\n",
        "    \n",
        "    def step(self, action_index):\n",
        "        \"\"\"\n",
        "        proceed env by 1 step = `operation_timestep_width` seconds\n",
        "        \"\"\"\n",
        "        \n",
        "        n_queue_veh_old = self.comp_n_veh_queue()\n",
        "        \n",
        "        #change signal by action\n",
        "        #decode action\n",
        "        binstr = f\"{action_index:04b}\"\n",
        "        i1, i2, i3, i4 = int(binstr[3]), int(binstr[2]), int(binstr[1]), int(binstr[0])\n",
        "        self.I1.signal_phase = i1\n",
        "        self.I1.signal_t = 0\n",
        "        self.I2.signal_phase = i2\n",
        "        self.I2.signal_t = 0\n",
        "        self.I3.signal_phase = i3\n",
        "        self.I3.signal_t = 0\n",
        "        self.I4.signal_phase = i4\n",
        "        self.I4.signal_t = 0\n",
        "        \n",
        "        #traffic dynamics. execute simulation for `operation_timestep_width` seconds\n",
        "        if self.W.check_simulation_ongoing():\n",
        "            self.W.exec_simulation(duration_t=self.operation_timestep_width)\n",
        "        \n",
        "        #observe state\n",
        "        observation = np.array(self.comp_state())\n",
        "        \n",
        "        #compute reward\n",
        "        n_queue_veh = self.comp_n_veh_queue()\n",
        "        reward = -(n_queue_veh-n_queue_veh_old)\n",
        "        \n",
        "        #check termination\n",
        "        done = False\n",
        "        if self.W.check_simulation_ongoing() == False:\n",
        "            done = True\n",
        "        \n",
        "        #log\n",
        "        self.log_state.append(observation)\n",
        "        self.log_reward.append(reward)\n",
        "        \n",
        "        return observation, reward, done, False, {}\n",
        "    \n",
        "    def render(self) -> None:\n",
        "        self.W.analyzer.network(self.W.TIME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# train_envs = gym.make('CartPole-v1')\n",
        "# test_envs = gym.make('CartPole-v1')\n",
        "\n",
        "max_steps = 3600\n",
        "time_per_step = 10\n",
        "\n",
        "env = TrafficSim(max_steps, time_per_step)\n",
        "\n",
        "train_envs = TrafficSim(max_steps, time_per_step)\n",
        "test_envs  = TrafficSim(max_steps, time_per_step)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMUNPN5SI_kd",
        "outputId": "7d68323c-0322-4b82-dafb-7c7f63e7a26d"
      },
      "outputs": [],
      "source": [
        "train_envs = ts.env.DummyVectorEnv([TrafficSim for _ in range(4)])\n",
        "test_envs = ts.env.DummyVectorEnv([TrafficSim for _ in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import torch, numpy as np\n",
        "from torch import nn\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, state_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, 128), nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, np.prod(action_shape)),\n",
        "        )\n",
        "\n",
        "    def forward(self, obs, state=None, info={}):\n",
        "        if not isinstance(obs, torch.Tensor):\n",
        "            obs = torch.tensor(obs, dtype=torch.float)\n",
        "        obs = obs.to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        batch = obs.shape[0]\n",
        "        logits = self.model(obs.view(batch, -1))\n",
        "        return logits, state\n",
        "\n",
        "state_shape = env.observation_space.shape or env.observation_space.n\n",
        "action_shape = env.action_space.shape or env.action_space.n\n",
        "net = Net(state_shape, action_shape).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "optim = torch.optim.Adam(net.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "policy = ts.policy.DQNPolicy(\n",
        "    model=net,\n",
        "    optim=optim,\n",
        "    action_space=env.action_space,\n",
        "    discount_factor=0.9,\n",
        "    estimation_step=3,\n",
        "    target_update_freq=320\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(20000, 4), exploration_noise=True)\n",
        "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #1: 3601it [00:22, 156.70it/s, env_step=3600, len=360, loss=774.003, n/ep=0, n/st=4, rew=-1065.00]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #1: test_reward: -807.500000 ± 443.966496, best_reward: -807.500000 ± 443.966496 in #1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #2: 3601it [00:22, 157.43it/s, env_step=7200, len=360, loss=829.938, n/ep=4, n/st=4, rew=-1117.50]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #2: test_reward: -1365.500000 ± 237.238804, best_reward: -807.500000 ± 443.966496 in #1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #3: 3601it [00:23, 153.47it/s, env_step=10800, len=360, loss=891.069, n/ep=0, n/st=4, rew=-1268.75]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #3: test_reward: -1219.000000 ± 278.817144, best_reward: -807.500000 ± 443.966496 in #1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #4: 3601it [00:22, 158.57it/s, env_step=14400, len=360, loss=792.999, n/ep=4, n/st=4, rew=-833.75]                           \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #4: test_reward: -1323.500000 ± 316.725828, best_reward: -807.500000 ± 443.966496 in #1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #5: 3601it [00:24, 146.60it/s, env_step=18000, len=360, loss=825.078, n/ep=0, n/st=4, rew=-1373.75]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #5: test_reward: -1021.500000 ± 504.232337, best_reward: -807.500000 ± 443.966496 in #1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #6: 3601it [00:23, 155.31it/s, env_step=21600, len=360, loss=839.338, n/ep=4, n/st=4, rew=-768.75]                           \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #6: test_reward: -526.000000 ± 377.099456, best_reward: -526.000000 ± 377.099456 in #6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #7: 3601it [00:23, 153.64it/s, env_step=25200, len=360, loss=847.738, n/ep=0, n/st=4, rew=-1137.50]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #7: test_reward: -836.000000 ± 657.752993, best_reward: -526.000000 ± 377.099456 in #6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #8: 3601it [00:21, 165.46it/s, env_step=28800, len=360, loss=639.266, n/ep=4, n/st=4, rew=-608.75]                           \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #8: test_reward: -315.500000 ± 440.547671, best_reward: -315.500000 ± 440.547671 in #8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #9: 3601it [00:23, 153.78it/s, env_step=32400, len=360, loss=663.499, n/ep=0, n/st=4, rew=-1297.50]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #9: test_reward: -933.500000 ± 639.222379, best_reward: -315.500000 ± 440.547671 in #8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch #10: 3601it [00:23, 152.07it/s, env_step=36000, len=360, loss=656.406, n/ep=4, n/st=4, rew=-1052.50]                          \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #10: test_reward: -988.500000 ± 378.047947, best_reward: -315.500000 ± 440.547671 in #8\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'dict' object has no attribute 'timing'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 11\u001b[0m\n\u001b[0;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mOffpolicyTrainer(\n\u001b[0;32m      2\u001b[0m     policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[0;32m      3\u001b[0m     train_collector\u001b[38;5;241m=\u001b[39mtrain_collector,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      9\u001b[0m     stop_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m mean_rewards: mean_rewards \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mspec\u001b[38;5;241m.\u001b[39mreward_threshold\n\u001b[0;32m     10\u001b[0m )\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFinished training! Use \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtiming\u001b[49m\u001b[38;5;241m.\u001b[39mtotal_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'timing'"
          ]
        }
      ],
      "source": [
        "result = ts.trainer.OffpolicyTrainer(\n",
        "    policy=policy,\n",
        "    train_collector=train_collector,\n",
        "    test_collector=test_collector,\n",
        "    max_epoch=10, step_per_epoch=max_steps, step_per_collect=4,\n",
        "    update_per_step=1/4, episode_per_test=10, batch_size=4,\n",
        "    train_fn=lambda epoch, env_step: policy.set_eps(0.1),\n",
        "    test_fn=lambda epoch, env_step: policy.set_eps(0.05),\n",
        "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold\n",
        ").run()\n",
        "print(f'Finished training! Use {result.timing.total_time}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UserWarning: Single environment detected, wrap to DummyVectorEnv.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'n/ep': 1,\n",
              " 'n/st': 360,\n",
              " 'rews': array([-260.]),\n",
              " 'lens': array([360]),\n",
              " 'idxs': array([0]),\n",
              " 'rew': -260.0,\n",
              " 'len': 360.0,\n",
              " 'rew_std': 0.0,\n",
              " 'len_std': 0.0}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "policy.eval()\n",
        "policy.set_eps(0.05)\n",
        "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
        "collector.collect(n_episode=1, render=1/35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "ename": "AttributeError",
          "evalue": "'World' object has no attribute 'analyzer'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcollector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tianshou\\env\\venvs.py:416\u001b[0m, in \u001b[0;36mBaseVectorEnv.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_async \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_id) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are still stepping, cannot \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender them now.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m     )\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [w\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tianshou\\env\\venvs.py:416\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_async \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_id) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    413\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwaiting_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are still stepping, cannot \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    414\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrender them now.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    415\u001b[0m     )\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [w\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers]\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tianshou\\env\\worker\\dummy.py:49\u001b[0m, in \u001b[0;36mDummyEnvWorker.render\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "Cell \u001b[1;32mIn[2], line 162\u001b[0m, in \u001b[0;36mTrafficSim.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrender\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyzer\u001b[49m\u001b[38;5;241m.\u001b[39mnetwork(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW\u001b[38;5;241m.\u001b[39mTIME)\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'World' object has no attribute 'analyzer'"
          ]
        }
      ],
      "source": [
        "collector.env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "S3-tJZy35Ck_",
        "XfsuU2AAE52C",
        "p-7U_cwgF5Ej",
        "_j3aUJZQ7nml"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
