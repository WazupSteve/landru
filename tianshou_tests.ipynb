{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "from uxsim import *\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSim(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        traffic scenario: 4 signalized intersections as shown below:\n",
    "                N1  N2\n",
    "                |   |\n",
    "            W1--I1--I2--E1\n",
    "                |   |\n",
    "            W2--I3--I4--E2\n",
    "                |   |\n",
    "                S1  S2\n",
    "        Traffic demand is generated from each boundary node to all other boundary nodes.\n",
    "        action: to determine which direction should have greenlight for every 10 seconds for each intersection. 16 actions.\n",
    "            action 1: greenlight for I1: direction 0, I2: 0, I3: 0, I4: 0, where direction 0 is E<->W, 1 is N<->S.\n",
    "            action 2: greenlight for I1: 1, I2: 0, I3: 0, I4: 0\n",
    "            action 3: greenlight for I1: 0, I2: 1, I3: 0, I4: 0\n",
    "            action 4: greenlight for I1: 1, I2: 1, I3: 0, I4: 0\n",
    "            action 5: ...\n",
    "        state: number of waiting vehicles at each incoming link. 16 dimension.\n",
    "        reward: negative of difference of total waiting vehicles\n",
    "        \"\"\"\n",
    "        \n",
    "        #action\n",
    "        self.n_action = 2**4\n",
    "        self.action_space = gym.spaces.Discrete(self.n_action) \n",
    "        \n",
    "        #state\n",
    "        self.n_state = 4*4\n",
    "        low = np.array([0 for i in range(self.n_state)])\n",
    "        high = np.array([100 for i in range(self.n_state)])\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset the env\n",
    "        \"\"\"\n",
    "        seed = None #whether demand is always random or not\n",
    "        W = World(\n",
    "            name=\"\",\n",
    "            deltan=5,\n",
    "            tmax=4000,\n",
    "            print_mode=0, save_mode=0, show_mode=1,\n",
    "            random_seed=seed,\n",
    "            duo_update_time=600\n",
    "        )\n",
    "        random.seed(seed)\n",
    "\n",
    "        #network definition\n",
    "        I1 = W.addNode(\"I1\", 0, 0, signal=[60,60])\n",
    "        I2 = W.addNode(\"I2\", 1, 0, signal=[60,60])\n",
    "        I3 = W.addNode(\"I3\", 0, -1, signal=[60,60])\n",
    "        I4 = W.addNode(\"I4\", 1, -1, signal=[60,60])\n",
    "        W1 = W.addNode(\"W1\", -1, 0)\n",
    "        W2 = W.addNode(\"W2\", -1, -1)\n",
    "        E1 = W.addNode(\"E1\", 2, 0)\n",
    "        E2 = W.addNode(\"E2\", 2, -1)\n",
    "        N1 = W.addNode(\"N1\", 0, 1)\n",
    "        N2 = W.addNode(\"N2\", 1, 1)\n",
    "        S1 = W.addNode(\"S1\", 0, -2)\n",
    "        S2 = W.addNode(\"S2\", 1, -2)\n",
    "        #E <-> W direction: signal group 0\n",
    "        for n1,n2 in [[W1, I1], [I1, I2], [I2, E1], [W2, I3], [I3, I4], [I4, E2]]:\n",
    "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
    "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
    "        #N <-> S direction: signal group 1\n",
    "        for n1,n2 in [[N1, I1], [I1, I3], [I3, S1], [N2, I2], [I2, I4], [I4, S2]]:\n",
    "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
    "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
    "\n",
    "        # random demand definition\n",
    "        dt = 30\n",
    "        demand = 0.22\n",
    "        for n1, n2 in itertools.permutations([W1, W2, E1, E2, N1, N2, S1, S2], 2):\n",
    "            for t in range(0, 3600, dt):\n",
    "                W.adddemand(n1, n2, t, t+dt, random.uniform(0, demand))\n",
    "        \n",
    "        # store UXsim object for later re-use\n",
    "        self.W = W\n",
    "        self.I1 = I1\n",
    "        self.I2 = I2\n",
    "        self.I3 = I3\n",
    "        self.I4 = I4\n",
    "        self.INLINKS = list(self.I1.inlinks.values()) + list(self.I2.inlinks.values()) + list(self.I3.inlinks.values()) + list(self.I4.inlinks.values())\n",
    "        \n",
    "        #initial observation\n",
    "        observation = np.array([0 for i in range(self.n_state)])\n",
    "        \n",
    "        #log\n",
    "        self.log_state = []\n",
    "        self.log_reward = []\n",
    "        \n",
    "        return observation, {}\n",
    "    \n",
    "    def comp_state(self):\n",
    "        \"\"\"\n",
    "        compute the current state\n",
    "        \"\"\"\n",
    "        vehicles_per_links = {}\n",
    "        for l in self.INLINKS:\n",
    "            vehicles_per_links[l] = l.num_vehicles_queue #l.num_vehicles_queue: the number of vehicles in queue in link l\n",
    "        return list(vehicles_per_links.values())\n",
    "    \n",
    "    def comp_n_veh_queue(self):\n",
    "        return sum(self.comp_state())\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        \"\"\"\n",
    "        proceed env by 1 step = `operation_timestep_width` seconds\n",
    "        \"\"\"\n",
    "        operation_timestep_width = 10\n",
    "        \n",
    "        n_queue_veh_old = self.comp_n_veh_queue()\n",
    "        \n",
    "        #change signal by action\n",
    "        #decode action\n",
    "        binstr = f\"{action_index:04b}\"\n",
    "        i1, i2, i3, i4 = int(binstr[3]), int(binstr[2]), int(binstr[1]), int(binstr[0])\n",
    "        self.I1.signal_phase = i1\n",
    "        self.I1.signal_t = 0\n",
    "        self.I2.signal_phase = i2\n",
    "        self.I2.signal_t = 0\n",
    "        self.I3.signal_phase = i3\n",
    "        self.I3.signal_t = 0\n",
    "        self.I4.signal_phase = i4\n",
    "        self.I4.signal_t = 0\n",
    "        \n",
    "        #traffic dynamics. execute simulation for `operation_timestep_width` seconds\n",
    "        if self.W.check_simulation_ongoing():\n",
    "            self.W.exec_simulation(duration_t=operation_timestep_width)\n",
    "        \n",
    "        #observe state\n",
    "        observation = np.array(self.comp_state())\n",
    "        \n",
    "        #compute reward\n",
    "        n_queue_veh = self.comp_n_veh_queue()\n",
    "        reward = -(n_queue_veh-n_queue_veh_old)\n",
    "        \n",
    "        #check termination\n",
    "        done = False\n",
    "        if self.W.check_simulation_ongoing() == False:\n",
    "            done = True\n",
    "        \n",
    "        #log\n",
    "        self.log_state.append(observation)\n",
    "        self.log_reward.append(reward)\n",
    "        \n",
    "        return observation, reward, done, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'CartPole-v1'\n",
    "lr, epoch, batch_size = 1e-4, 200, 128\n",
    "train_num, test_num = 1, 1\n",
    "gamma, n_step, target_freq = 0.99, 3, 320\n",
    "buffer_size = 10000\n",
    "eps_train, eps_test = 0.9, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = gym.make(task)\n",
    "tfc = TrafficSim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.03626679, -0.00384594, -0.04223777,  0.04494417], dtype=float32), {})\n",
      "(array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), {})\n"
     ]
    }
   ],
   "source": [
    "print(cap.reset())\n",
    "print(tfc.reset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))\n",
    "# For other loggers, see https://tianshou.readthedocs.io/en/master/01_tutorials/05_logger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also try SubprocVectorEnv, which will use parallelization\n",
    "train_envs = ts.env.DummyVectorEnv([TrafficSim for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([TrafficSim for _ in range(test_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "# Note: You can easily define other networks.\n",
    "# See https://tianshou.readthedocs.io/en/master/01_tutorials/00_dqn.html#build-the-network\n",
    "env = TrafficSim()\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[128, 128, 128])\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    discount_factor=gamma, \n",
    "    action_space=env.action_space,\n",
    "    estimation_step=1,\n",
    "    target_update_freq=target_freq\n",
    ")\n",
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 301it [00:01, 172.72it/s, env_step=300, gradient_step=30, len=0, n/ep=0, n/st=10, rew=0.00]                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: -1505.000000 ± 0.000000, best_reward: -1505.000000 ± 0.000000 in #1\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'reward_threshold'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 15\u001b[0m\n\u001b[0;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOffpolicyTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_collector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_per_collect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepisode_per_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_num\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mupdate_per_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mstep_per_collect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_step\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps_train\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_step\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_eps\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmean_rewards\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean_rewards\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m---> 15\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mtiming\u001b[38;5;241m.\u001b[39mtotal_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\trainer\\base.py:526\u001b[0m, in \u001b[0;36mBaseTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_run \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m     \u001b[43mdeque\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# feed the entire iterator into a zero-length deque\u001b[39;00m\n\u001b[0;32m    527\u001b[0m     info \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[0;32m    528\u001b[0m         start_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time,\n\u001b[0;32m    529\u001b[0m         policy_update_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_update_time,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    534\u001b[0m         test_collector\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\trainer\\base.py:359\u001b[0m, in \u001b[0;36mBaseTrainer.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;66;03m# test\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 359\u001b[0m         test_stat, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    361\u001b[0m info_stat \u001b[38;5;241m=\u001b[39m gather_info(\n\u001b[0;32m    362\u001b[0m     start_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_time,\n\u001b[0;32m    363\u001b[0m     policy_update_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_update_time,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m     test_collector\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_collector,\n\u001b[0;32m    369\u001b[0m )\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog_info_data(asdict(info_stat), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch)\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\trainer\\base.py:416\u001b[0m, in \u001b[0;36mBaseTrainer.test_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[0;32m    414\u001b[0m     \u001b[38;5;28mprint\u001b[39m(log_msg, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_reward\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    417\u001b[0m     stop_fn_flag \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m test_stat, stop_fn_flag\n",
      "Cell \u001b[1;32mIn[56], line 13\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(mean_rewards)\u001b[0m\n\u001b[0;32m      1\u001b[0m result \u001b[38;5;241m=\u001b[39m ts\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mOffpolicyTrainer(\n\u001b[0;32m      2\u001b[0m     policy\u001b[38;5;241m=\u001b[39mpolicy,\n\u001b[0;32m      3\u001b[0m     train_collector\u001b[38;5;241m=\u001b[39mtrain_collector,\n\u001b[0;32m      4\u001b[0m     test_collector\u001b[38;5;241m=\u001b[39mtest_collector,\n\u001b[0;32m      5\u001b[0m     max_epoch\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m      6\u001b[0m     step_per_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[0;32m      7\u001b[0m     step_per_collect\u001b[38;5;241m=\u001b[39mstep_per_collect,\n\u001b[0;32m      8\u001b[0m     episode_per_test\u001b[38;5;241m=\u001b[39mtest_num,\n\u001b[0;32m      9\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     10\u001b[0m     update_per_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m step_per_collect,\n\u001b[0;32m     11\u001b[0m     train_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, env_step: policy\u001b[38;5;241m.\u001b[39mset_eps(eps_train),\n\u001b[0;32m     12\u001b[0m     test_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m epoch, env_step: policy\u001b[38;5;241m.\u001b[39mset_eps(eps_test),\n\u001b[1;32m---> 13\u001b[0m     stop_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m mean_rewards: mean_rewards \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreward_threshold\u001b[49m,\n\u001b[0;32m     14\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[0;32m     15\u001b[0m )\u001b[38;5;241m.\u001b[39mrun()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished training in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mtiming\u001b[38;5;241m.\u001b[39mtotal_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'reward_threshold'"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=epoch,\n",
    "    step_per_epoch=300,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=test_num,\n",
    "    batch_size=batch_size,\n",
    "    update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger,\n",
    ").run()\n",
    "print(f\"Finished training in {result.timing.total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\data\\collector.py:101: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CollectStats(n_collected_episodes=1, n_collected_steps=180, collect_time=9.335633993148804, collect_speed=19.280961542847294, returns=array([180.]), returns_stat=SequenceSummaryStats(mean=180.0, std=0.0, max=180.0, min=180.0), lens=array([180]), lens_stat=SequenceSummaryStats(mean=180.0, std=0.0, max=180.0, min=180.0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(eps_test)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
