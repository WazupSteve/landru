{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "from uxsim import *\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSim(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        traffic scenario: 4 signalized intersections as shown below:\n",
    "                N1  N2\n",
    "                |   |\n",
    "            W1--I1--I2--E1\n",
    "                |   |\n",
    "            W2--I3--I4--E2\n",
    "                |   |\n",
    "                S1  S2\n",
    "        Traffic demand is generated from each boundary node to all other boundary nodes.\n",
    "        action: to determine which direction should have greenlight for every 10 seconds for each intersection. 16 actions.\n",
    "            action 1: greenlight for I1: direction 0, I2: 0, I3: 0, I4: 0, where direction 0 is E<->W, 1 is N<->S.\n",
    "            action 2: greenlight for I1: 1, I2: 0, I3: 0, I4: 0\n",
    "            action 3: greenlight for I1: 0, I2: 1, I3: 0, I4: 0\n",
    "            action 4: greenlight for I1: 1, I2: 1, I3: 0, I4: 0\n",
    "            action 5: ...\n",
    "        state: number of waiting vehicles at each incoming link. 16 dimension.\n",
    "        reward: negative of difference of total waiting vehicles\n",
    "        \"\"\"\n",
    "        \n",
    "        #action\n",
    "        self.n_action = 2**4\n",
    "        self.action_space = gym.spaces.Discrete(self.n_action) \n",
    "        \n",
    "        #state\n",
    "        self.n_state = 4*4\n",
    "        low = np.array([0 for i in range(self.n_state)])\n",
    "        high = np.array([100 for i in range(self.n_state)])\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset the env\n",
    "        \"\"\"\n",
    "        seed = None #whether demand is always random or not\n",
    "        W = World(\n",
    "            name=\"\",\n",
    "            deltan=5,\n",
    "            tmax=4000,\n",
    "            print_mode=0, save_mode=0, show_mode=1,\n",
    "            random_seed=seed,\n",
    "            duo_update_time=600\n",
    "        )\n",
    "        random.seed(seed)\n",
    "\n",
    "        #network definition\n",
    "        I1 = W.addNode(\"I1\", 0, 0, signal=[60,60])\n",
    "        I2 = W.addNode(\"I2\", 1, 0, signal=[60,60])\n",
    "        I3 = W.addNode(\"I3\", 0, -1, signal=[60,60])\n",
    "        I4 = W.addNode(\"I4\", 1, -1, signal=[60,60])\n",
    "        W1 = W.addNode(\"W1\", -1, 0)\n",
    "        W2 = W.addNode(\"W2\", -1, -1)\n",
    "        E1 = W.addNode(\"E1\", 2, 0)\n",
    "        E2 = W.addNode(\"E2\", 2, -1)\n",
    "        N1 = W.addNode(\"N1\", 0, 1)\n",
    "        N2 = W.addNode(\"N2\", 1, 1)\n",
    "        S1 = W.addNode(\"S1\", 0, -2)\n",
    "        S2 = W.addNode(\"S2\", 1, -2)\n",
    "        #E <-> W direction: signal group 0\n",
    "        for n1,n2 in [[W1, I1], [I1, I2], [I2, E1], [W2, I3], [I3, I4], [I4, E2]]:\n",
    "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
    "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
    "        #N <-> S direction: signal group 1\n",
    "        for n1,n2 in [[N1, I1], [I1, I3], [I3, S1], [N2, I2], [I2, I4], [I4, S2]]:\n",
    "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
    "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
    "\n",
    "        # random demand definition\n",
    "        dt = 30\n",
    "        demand = 0.22\n",
    "        for n1, n2 in itertools.permutations([W1, W2, E1, E2, N1, N2, S1, S2], 2):\n",
    "            for t in range(0, 3600, dt):\n",
    "                W.adddemand(n1, n2, t, t+dt, random.uniform(0, demand))\n",
    "        \n",
    "        # store UXsim object for later re-use\n",
    "        self.W = W\n",
    "        self.I1 = I1\n",
    "        self.I2 = I2\n",
    "        self.I3 = I3\n",
    "        self.I4 = I4\n",
    "        self.INLINKS = list(self.I1.inlinks.values()) + list(self.I2.inlinks.values()) + list(self.I3.inlinks.values()) + list(self.I4.inlinks.values())\n",
    "        \n",
    "        #initial observation\n",
    "        observation = np.array([0 for i in range(self.n_state)])\n",
    "        \n",
    "        #log\n",
    "        self.log_state = []\n",
    "        self.log_reward = []\n",
    "        \n",
    "        return observation, None\n",
    "    \n",
    "    def comp_state(self):\n",
    "        \"\"\"\n",
    "        compute the current state\n",
    "        \"\"\"\n",
    "        vehicles_per_links = {}\n",
    "        for l in self.INLINKS:\n",
    "            vehicles_per_links[l] = l.num_vehicles_queue #l.num_vehicles_queue: the number of vehicles in queue in link l\n",
    "        return list(vehicles_per_links.values())\n",
    "    \n",
    "    def comp_n_veh_queue(self):\n",
    "        return sum(self.comp_state())\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        \"\"\"\n",
    "        proceed env by 1 step = `operation_timestep_width` seconds\n",
    "        \"\"\"\n",
    "        operation_timestep_width = 10\n",
    "        \n",
    "        n_queue_veh_old = self.comp_n_veh_queue()\n",
    "        \n",
    "        #change signal by action\n",
    "        #decode action\n",
    "        binstr = f\"{action_index:04b}\"\n",
    "        i1, i2, i3, i4 = int(binstr[3]), int(binstr[2]), int(binstr[1]), int(binstr[0])\n",
    "        self.I1.signal_phase = i1\n",
    "        self.I1.signal_t = 0\n",
    "        self.I2.signal_phase = i2\n",
    "        self.I2.signal_t = 0\n",
    "        self.I3.signal_phase = i3\n",
    "        self.I3.signal_t = 0\n",
    "        self.I4.signal_phase = i4\n",
    "        self.I4.signal_t = 0\n",
    "        \n",
    "        #traffic dynamics. execute simulation for `operation_timestep_width` seconds\n",
    "        if self.W.check_simulation_ongoing():\n",
    "            self.W.exec_simulation(duration_t=operation_timestep_width)\n",
    "        \n",
    "        #observe state\n",
    "        observation = np.array(self.comp_state())\n",
    "        \n",
    "        #compute reward\n",
    "        n_queue_veh = self.comp_n_veh_queue()\n",
    "        reward = -(n_queue_veh-n_queue_veh_old)\n",
    "        \n",
    "        #check termination\n",
    "        done = False\n",
    "        if self.W.check_simulation_ongoing() == False:\n",
    "            done = True\n",
    "        \n",
    "        #log\n",
    "        self.log_state.append(observation)\n",
    "        self.log_reward.append(reward)\n",
    "        \n",
    "        return observation, reward, done, {}, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'CartPole-v1'\n",
    "lr, epoch, batch_size = 1e-3, 10, 64\n",
    "train_num, test_num = 10, 100\n",
    "gamma, n_step, target_freq = 0.9, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.1, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))\n",
    "# For other loggers, see https://tianshou.readthedocs.io/en/master/01_tutorials/05_logger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also try SubprocVectorEnv, which will use parallelization\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: TrafficSim() for _ in range(train_num)])\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: TrafficSim() for _ in range(test_num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "# Note: You can easily define other networks.\n",
    "# See https://tianshou.readthedocs.io/en/master/01_tutorials/00_dqn.html#build-the-network\n",
    "env = gym.make(task, render_mode=\"human\")\n",
    "state_shape = env.observation_space.shape or env.observation_space.n\n",
    "action_shape = env.action_space.shape or env.action_space.n\n",
    "net = Net(state_shape=state_shape, action_shape=action_shape, hidden_sizes=[128, 128, 128])\n",
    "optim = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    discount_factor=gamma, \n",
    "    action_space=env.action_space,\n",
    "    estimation_step=n_step,\n",
    "    target_update_freq=target_freq\n",
    ")\n",
    "train_collector = ts.data.Collector(policy, train_envs, ts.data.VectorReplayBuffer(buffer_size, train_num), exploration_noise=True)\n",
    "test_collector = ts.data.Collector(policy, test_envs, exploration_noise=True)  # because DQN uses epsilon-greedy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #1: 10001it [00:09, 1075.03it/s, env_step=10000, gradient_step=1000, len=193, n/ep=0, n/st=10, rew=193.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: test_reward: 207.510000 ± 29.486775, best_reward: 207.510000 ± 29.486775 in #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #2: 10001it [00:12, 814.13it/s, env_step=20000, gradient_step=2000, len=232, n/ep=0, n/st=10, rew=232.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #2: test_reward: 350.980000 ± 75.211699, best_reward: 350.980000 ± 75.211699 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #3: 10001it [00:10, 938.22it/s, env_step=30000, gradient_step=3000, len=273, n/ep=0, n/st=10, rew=273.00]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #3: test_reward: 254.730000 ± 24.105126, best_reward: 350.980000 ± 75.211699 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #4: 10001it [00:08, 1132.21it/s, env_step=40000, gradient_step=4000, len=256, n/ep=0, n/st=10, rew=256.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #4: test_reward: 256.900000 ± 24.863829, best_reward: 350.980000 ± 75.211699 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #5: 10001it [00:08, 1184.92it/s, env_step=50000, gradient_step=5000, len=245, n/ep=0, n/st=10, rew=245.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #5: test_reward: 245.920000 ± 19.266904, best_reward: 350.980000 ± 75.211699 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #6: 10001it [00:08, 1141.69it/s, env_step=60000, gradient_step=6000, len=237, n/ep=0, n/st=10, rew=237.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #6: test_reward: 270.570000 ± 24.397645, best_reward: 350.980000 ± 75.211699 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #7: 10001it [00:08, 1128.99it/s, env_step=70000, gradient_step=7000, len=213, n/ep=0, n/st=10, rew=213.00]                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #7: test_reward: 226.330000 ± 10.177480, best_reward: 350.980000 ± 75.211699 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #8: 10001it [00:08, 1122.28it/s, env_step=80000, gradient_step=8000, len=239, n/ep=0, n/st=10, rew=239.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #8: test_reward: 213.930000 ± 8.835446, best_reward: 350.980000 ± 75.211699 in #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #9: 10001it [00:09, 1046.32it/s, env_step=90000, gradient_step=9000, len=343, n/ep=0, n/st=10, rew=343.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #9: test_reward: 468.430000 ± 48.172659, best_reward: 468.430000 ± 48.172659 in #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch #10: 10001it [00:21, 457.50it/s, env_step=100000, gradient_step=10000, len=191, n/ep=0, n/st=10, rew=191.00]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #10: test_reward: 176.160000 ± 9.569451, best_reward: 468.430000 ± 48.172659 in #9\n",
      "Finished training in 127.54945468902588 seconds\n"
     ]
    }
   ],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=epoch,\n",
    "    step_per_epoch=step_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=test_num,\n",
    "    batch_size=batch_size,\n",
    "    update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger,\n",
    ").run()\n",
    "print(f\"Finished training in {result.timing.total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\data\\collector.py:101: UserWarning: Single environment detected, wrap to DummyVectorEnv.\n",
      "  warnings.warn(\"Single environment detected, wrap to DummyVectorEnv.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CollectStats(n_collected_episodes=1, n_collected_steps=180, collect_time=9.335633993148804, collect_speed=19.280961542847294, returns=array([180.]), returns_stat=SequenceSummaryStats(mean=180.0, std=0.0, max=180.0, min=180.0), lens=array([180]), lens_stat=SequenceSummaryStats(mean=180.0, std=0.0, max=180.0, min=180.0))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(eps_test)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
