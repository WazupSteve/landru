{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import tianshou as ts\n",
    "from uxsim import *\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = ts.utils.TensorboardLogger(SummaryWriter('log/dqn'))\n",
    "# For other loggers, see https://tianshou.readthedocs.io/en/master/01_tutorials/05_logger.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSim(gym.Env):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        traffic scenario: 4 signalized intersections as shown below:\n",
    "                N1  N2\n",
    "                |   |\n",
    "            W1--I1--I2--E1\n",
    "                |   |\n",
    "            W2--I3--I4--E2\n",
    "                |   |\n",
    "                S1  S2\n",
    "        Traffic demand is generated from each boundary node to all other boundary nodes.\n",
    "        action: to determine which direction should have greenlight for every 10 seconds for each intersection. 16 actions.\n",
    "            action 1: greenlight for I1: direction 0, I2: 0, I3: 0, I4: 0, where direction 0 is E<->W, 1 is N<->S.\n",
    "            action 2: greenlight for I1: 1, I2: 0, I3: 0, I4: 0\n",
    "            action 3: greenlight for I1: 0, I2: 1, I3: 0, I4: 0\n",
    "            action 4: greenlight for I1: 1, I2: 1, I3: 0, I4: 0\n",
    "            action 5: ...\n",
    "        state: number of waiting vehicles at each incoming link. 16 dimension.\n",
    "        reward: negative of difference of total waiting vehicles\n",
    "        \"\"\"\n",
    "        \n",
    "        #action\n",
    "        self.n_action = 2**4\n",
    "        self.action_space = gym.spaces.Discrete(self.n_action) \n",
    "        \n",
    "        #state\n",
    "        self.n_state = 4*4\n",
    "        low = np.array([0 for i in range(self.n_state)])\n",
    "        high = np.array([100 for i in range(self.n_state)])\n",
    "        self.observation_space = gym.spaces.Box(low=low, high=high)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        reset the env\n",
    "        \"\"\"\n",
    "        seed = None #whether demand is always random or not\n",
    "        W = World(\n",
    "            name=\"\",\n",
    "            deltan=5,\n",
    "            tmax=4000,\n",
    "            print_mode=0, save_mode=0, show_mode=1,\n",
    "            random_seed=seed,\n",
    "            duo_update_time=600\n",
    "        )\n",
    "        random.seed(seed)\n",
    "\n",
    "        #network definition\n",
    "        I1 = W.addNode(\"I1\", 0, 0, signal=[60,60])\n",
    "        I2 = W.addNode(\"I2\", 1, 0, signal=[60,60])\n",
    "        I3 = W.addNode(\"I3\", 0, -1, signal=[60,60])\n",
    "        I4 = W.addNode(\"I4\", 1, -1, signal=[60,60])\n",
    "        W1 = W.addNode(\"W1\", -1, 0)\n",
    "        W2 = W.addNode(\"W2\", -1, -1)\n",
    "        E1 = W.addNode(\"E1\", 2, 0)\n",
    "        E2 = W.addNode(\"E2\", 2, -1)\n",
    "        N1 = W.addNode(\"N1\", 0, 1)\n",
    "        N2 = W.addNode(\"N2\", 1, 1)\n",
    "        S1 = W.addNode(\"S1\", 0, -2)\n",
    "        S2 = W.addNode(\"S2\", 1, -2)\n",
    "        #E <-> W direction: signal group 0\n",
    "        for n1,n2 in [[W1, I1], [I1, I2], [I2, E1], [W2, I3], [I3, I4], [I4, E2]]:\n",
    "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
    "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=0)\n",
    "        #N <-> S direction: signal group 1\n",
    "        for n1,n2 in [[N1, I1], [I1, I3], [I3, S1], [N2, I2], [I2, I4], [I4, S2]]:\n",
    "            W.addLink(n1.name+n2.name, n1, n2, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
    "            W.addLink(n2.name+n1.name, n2, n1, length=500, free_flow_speed=10, jam_density=0.2, signal_group=1)\n",
    "\n",
    "        # random demand definition\n",
    "        dt = 30\n",
    "        demand = 0.22\n",
    "        for n1, n2 in itertools.permutations([W1, W2, E1, E2, N1, N2, S1, S2], 2):\n",
    "            for t in range(0, 3600, dt):\n",
    "                W.adddemand(n1, n2, t, t+dt, random.uniform(0, demand))\n",
    "        \n",
    "        # store UXsim object for later re-use\n",
    "        self.W = W\n",
    "        self.I1 = I1\n",
    "        self.I2 = I2\n",
    "        self.I3 = I3\n",
    "        self.I4 = I4\n",
    "        self.INLINKS = list(self.I1.inlinks.values()) + list(self.I2.inlinks.values()) + list(self.I3.inlinks.values()) + list(self.I4.inlinks.values())\n",
    "        \n",
    "        #initial observation\n",
    "        observation = np.array([0 for i in range(self.n_state)])\n",
    "        \n",
    "        #log\n",
    "        self.log_state = []\n",
    "        self.log_reward = []\n",
    "        \n",
    "        return observation, {}\n",
    "    \n",
    "    def comp_state(self):\n",
    "        \"\"\"\n",
    "        compute the current state\n",
    "        \"\"\"\n",
    "        vehicles_per_links = {}\n",
    "        for l in self.INLINKS:\n",
    "            vehicles_per_links[l] = l.num_vehicles_queue #l.num_vehicles_queue: the number of vehicles in queue in link l\n",
    "        return list(vehicles_per_links.values())\n",
    "    \n",
    "    def comp_n_veh_queue(self):\n",
    "        return sum(self.comp_state())\n",
    "    \n",
    "    def step(self, action_index):\n",
    "        \"\"\"\n",
    "        proceed env by 1 step = `operation_timestep_width` seconds\n",
    "        \"\"\"\n",
    "        operation_timestep_width = 10\n",
    "        \n",
    "        n_queue_veh_old = self.comp_n_veh_queue()\n",
    "        \n",
    "        #change signal by action\n",
    "        #decode action\n",
    "        binstr = f\"{action_index:04b}\"\n",
    "        i1, i2, i3, i4 = int(binstr[3]), int(binstr[2]), int(binstr[1]), int(binstr[0])\n",
    "        self.I1.signal_phase = i1\n",
    "        self.I1.signal_t = 0\n",
    "        self.I2.signal_phase = i2\n",
    "        self.I2.signal_t = 0\n",
    "        self.I3.signal_phase = i3\n",
    "        self.I3.signal_t = 0\n",
    "        self.I4.signal_phase = i4\n",
    "        self.I4.signal_t = 0\n",
    "        \n",
    "        #traffic dynamics. execute simulation for `operation_timestep_width` seconds\n",
    "        if self.W.check_simulation_ongoing():\n",
    "            self.W.exec_simulation(duration_t=operation_timestep_width)\n",
    "        \n",
    "        #observe state\n",
    "        observation = np.array(self.comp_state())\n",
    "        \n",
    "        #compute reward\n",
    "        n_queue_veh = self.comp_n_veh_queue()\n",
    "        reward = -(n_queue_veh-n_queue_veh_old)\n",
    "        \n",
    "        #check termination\n",
    "        done = False\n",
    "        if self.W.check_simulation_ongoing() == False:\n",
    "            print(\"Done simulating\")\n",
    "            done = True\n",
    "        \n",
    "        #log\n",
    "        self.log_state.append(observation)\n",
    "        self.log_reward.append(reward)\n",
    "        \n",
    "        return observation, reward, done, False, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'CartPole-v1'\n",
    "lr, epoch, batch_size = 1e-4, 200, 128\n",
    "train_num, test_num = 1, 1\n",
    "gamma, n_step, target_freq = 0.99, 3, 320\n",
    "buffer_size = 20000\n",
    "eps_train, eps_test = 0.9, 0.05\n",
    "step_per_epoch, step_per_collect = 10000, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tianshou.utils.net.discrete import Actor\n",
    "from tianshou.data import Collector, VectorReplayBuffer\n",
    "from tianshou.utils.net.common import Net\n",
    "\n",
    "train_env_num = 4\n",
    "buffer_size = (\n",
    "    2000  # Since REINFORCE is an on-policy algorithm, we don't need a very large buffer size\n",
    ")\n",
    "\n",
    "# Create the environments, used for training and evaluation\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "test_envs = ts.env.DummyVectorEnv([lambda: gym.make(\"CartPole-v1\") for _ in range(2)])\n",
    "train_envs = ts.env.DummyVectorEnv([lambda: gym.make(\"CartPole-v1\") for _ in range(train_env_num)])\n",
    "\n",
    "# Create the Policy instance\n",
    "assert env.observation_space.shape is not None\n",
    "net = Net(\n",
    "    env.observation_space.shape,\n",
    "    hidden_sizes=[\n",
    "        16,\n",
    "    ],\n",
    ")\n",
    "\n",
    "assert isinstance(env.action_space, gym.spaces.Discrete)\n",
    "actor = Actor(net, env.action_space.n)\n",
    "optim = torch.optim.Adam(actor.parameters(), lr=0.001)\n",
    "\n",
    "policy = ts.policy.DQNPolicy(\n",
    "    model=net,\n",
    "    optim=optim,\n",
    "    discount_factor=gamma, \n",
    "    action_space=env.action_space,\n",
    "    estimation_step=n_step,\n",
    "    target_update_freq=target_freq\n",
    ")\n",
    "\n",
    "# Create the replay buffer and the collector\n",
    "replayBuffer = VectorReplayBuffer(buffer_size, train_env_num)\n",
    "test_collector = Collector(policy, test_envs)\n",
    "train_collector = Collector(policy, train_envs, replayBuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "13 (<class 'numpy.int64'>) invalid",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m n_episode \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episode):\n\u001b[1;32m----> 9\u001b[0m     evaluation_result \u001b[38;5;241m=\u001b[39m \u001b[43mtest_collector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_episode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation mean episodic reward is: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluation_result\u001b[38;5;241m.\u001b[39mreturns\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m     train_collector\u001b[38;5;241m.\u001b[39mcollect(n_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\data\\collector.py:317\u001b[0m, in \u001b[0;36mCollector.collect\u001b[1;34m(self, n_step, n_episode, random, render, no_grad, gym_reset_kwargs)\u001b[0m\n\u001b[0;32m    314\u001b[0m action_remap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mmap_action(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mact)\n\u001b[0;32m    315\u001b[0m \u001b[38;5;66;03m# step in env\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m obs_next, rew, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_remap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mready_env_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m done \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or(terminated, truncated)\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    324\u001b[0m     obs_next\u001b[38;5;241m=\u001b[39mobs_next,\n\u001b[0;32m    325\u001b[0m     rew\u001b[38;5;241m=\u001b[39mrew,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    329\u001b[0m     info\u001b[38;5;241m=\u001b[39minfo,\n\u001b[0;32m    330\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\env\\venvs.py:276\u001b[0m, in \u001b[0;36mBaseVectorEnv.step\u001b[1;34m(self, action, id)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(action) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mid\u001b[39m)\n\u001b[0;32m    275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mid\u001b[39m):\n\u001b[1;32m--> 276\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mid\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\tianshou\\env\\worker\\dummy.py:41\u001b[0m, in \u001b[0;36mDummyEnvWorker.send\u001b[1;34m(self, action, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mreset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresult \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:49\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lalit\\source\\landru\\venv\\Lib\\site-packages\\gymnasium\\envs\\classic_control\\cartpole.py:133\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mcontains(\n\u001b[0;32m    134\u001b[0m         action\n\u001b[0;32m    135\u001b[0m     ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maction\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(action)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) invalid\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCall reset before using step method.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    137\u001b[0m     x, x_dot, theta, theta_dot \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\n",
      "\u001b[1;31mAssertionError\u001b[0m: 13 (<class 'numpy.int64'>) invalid"
     ]
    }
   ],
   "source": [
    "train_collector.reset()\n",
    "train_envs.reset()\n",
    "test_collector.reset()\n",
    "test_envs.reset()\n",
    "replayBuffer.reset()\n",
    "\n",
    "n_episode = 20\n",
    "for _i in range(n_episode):\n",
    "    evaluation_result = test_collector.collect(n_episode=n_episode)\n",
    "    print(f\"Evaluation mean episodic reward is: {evaluation_result.returns.mean()}\")\n",
    "    train_collector.collect(n_step=2000)\n",
    "    # 0 means taking all data stored in train_collector.buffer\n",
    "    policy.update(sample_size=None, buffer=train_collector.buffer, batch_size=512, repeat=1)\n",
    "    train_collector.reset_buffer(keep_statistics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = ts.trainer.OffpolicyTrainer(\n",
    "    policy=policy,\n",
    "    train_collector=train_collector,\n",
    "    test_collector=test_collector,\n",
    "    max_epoch=epoch,\n",
    "    step_per_epoch=step_per_epoch,\n",
    "    step_per_collect=step_per_collect,\n",
    "    episode_per_test=test_num,\n",
    "    batch_size=batch_size,\n",
    "    update_per_step=1 / step_per_collect,\n",
    "    train_fn=lambda epoch, env_step: policy.set_eps(eps_train),\n",
    "    test_fn=lambda epoch, env_step: policy.set_eps(eps_test),\n",
    "    stop_fn=lambda mean_rewards: mean_rewards >= env.spec.reward_threshold,\n",
    "    logger=logger,\n",
    ").run()\n",
    "print(f\"Finished training in {result.timing.total_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(), 'dqn.pth')\n",
    "policy.load_state_dict(torch.load('dqn.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.eval()\n",
    "policy.set_eps(eps_test)\n",
    "collector = ts.data.Collector(policy, env, exploration_noise=True)\n",
    "collector.collect(n_episode=1, render=1 / 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
